# ============================================================================
# ForgeComply 360 - Docker Compose (On-Prem Deployment)
# Forge Cyber Defense - SDVOSB
#
# Usage:
#   cp .env.example .env  (edit with your settings)
#   docker compose up -d
#   Open https://localhost
# ============================================================================

services:
  # ── API Server ──────────────────────────────────────────────────
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.api
    container_name: forgecomply-api
    restart: unless-stopped
    ports:
      - "${API_PORT:-8787}:8787"
    volumes:
      - forge-data:/data
    environment:
      - PORT=8787
      - DATA_DIR=/data
      - JWT_SECRET=${JWT_SECRET:?FATAL: JWT_SECRET must be set in .env file. Generate with: openssl rand -hex 64}
      - CORS_ORIGIN=${CORS_ORIGIN:-http://localhost:3000}
      - ENVIRONMENT=on-prem
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1:8b}
    depends_on:
      ollama:
        condition: service_started
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
    networks:
      - forge-network

  # ── Frontend (nginx + TLS) ─────────────────────────────────────
  frontend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.frontend
    container_name: forgecomply-frontend
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-443}:443"
      - "80:80"
    volumes:
      - ${TLS_CERT_DIR:-./certs}:/etc/nginx/certs:ro
    depends_on:
      - api
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M
    networks:
      - forge-network

  # ── Ollama (Local AI) ──────────────────────────────────────────
  ollama:
    image: ollama/ollama:latest
    container_name: forgecomply-ollama
    restart: unless-stopped
    # No host port exposure — only accessible via internal Docker network
    # at http://ollama:11434. Uncomment 'ports' below if you need direct access.
    # ports:
    #   - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
    # Uncomment for GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    networks:
      - forge-network

volumes:
  forge-data:
    driver: local
  ollama-models:
    driver: local

networks:
  forge-network:
    driver: bridge
